# æ¨¡å— 04ï¼šåˆ†æ®µå†…å®¹æå– â­â­â­

[â† è¿”å›ä¸»æ–‡æ¡£](../AI_AGENT_è§†è§‰è¿˜åŸ_SOP_3.1_ä¸»æ–‡æ¡£.md)

---

## ğŸ¯ ç›®æ ‡

**é€ä¸ªsectionæå–å®Œæ•´HTMLå†…å®¹** â€” è¿™æ˜¯ç¡®ä¿è§†è§‰è¿˜åŸåº¦çš„**æœ€å…³é”®æ­¥éª¤**ï¼

---

## âš ï¸ å…³é”®è¦æ±‚

- âœ… æ¯ä¸ªsectionå¿…é¡»**å•ç‹¬å¤„ç†**
- âœ… **ä¸èƒ½ç”¨å ä½ç¬¦ä»£æ›¿**
- âœ… å¿…é¡»ä¿ç•™**æ‰€æœ‰åµŒå¥—ç»“æ„**
- âœ… å›¾ç‰‡ã€é“¾æ¥å¿…é¡»å®Œæ•´
- âœ… **æ‰€æœ‰æ–‡å­—å†…å®¹**å¿…é¡»æå–
- âœ… **æ‰€æœ‰data-*å±æ€§**å¿…é¡»ä¿ç•™

---

## ğŸ“‹ æ‰§è¡Œæµç¨‹

### æ•´ä½“æµç¨‹

```
FOR æ¯ä¸ª section (i = 1 to N):
  1. æ ¹æ®sections_structure.jsonè¯»å–è¯¥sectionçš„è¡Œå·èŒƒå›´
  2. ä»HTMLæ–‡ä»¶æå–è¯¥sectionçš„å®Œæ•´HTML
  3. è§£æHTMLç»“æ„
  4. æå–æ‰€æœ‰å†…å®¹ï¼š
     - æ–‡å­—å†…å®¹
     - å›¾ç‰‡URLå’Œaltæ–‡æœ¬
     - é“¾æ¥hrefå’Œæ–‡å­—
     - classNameå’Œç»“æ„
     - data-*å±æ€§
  5. ä¿å­˜ä¸º section_{i}_content.json
  6. ç»§ç»­ä¸‹ä¸€ä¸ªsection
END FOR
```

---

### æ­¥éª¤1ï¼šè¯»å–Section HTML

**ä½¿ç”¨sections_structure.jsonå®šä½section**

```python
import json

def read_section_html(html_file, section_info):
    """è¯»å–æŒ‡å®šsectionçš„HTMLå†…å®¹"""
    with open(html_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    start = section_info['start'] - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
    end = section_info['end']
    
    section_html = ''.join(lines[start:end])
    
    return section_html

# åŠ è½½sectionåœ°å›¾
with open('sections_structure.json', 'r') as f:
    structure = json.load(f)

# ç¤ºä¾‹ï¼šè¯»å–ç¬¬2ä¸ªsection
section_2_info = structure['sections_map'][1]  # index=2, ä½†æ•°ç»„ç´¢å¼•ä¸º1
section_2_html = read_section_html('index.html', section_2_info)
```

---

### æ­¥éª¤2ï¼šè§£æHTMLç»“æ„

**ä½¿ç”¨BeautifulSoupè§£æ**

```python
from bs4 import BeautifulSoup

def parse_section_content(html):
    """è§£æsectionçš„å®Œæ•´å†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')
    section = soup.find('section')
    
    if not section:
        return None
    
    content = {
        'tag': 'section',
        'attributes': dict(section.attrs),
        'children': []
    }
    
    # é€’å½’è§£ææ‰€æœ‰å­å…ƒç´ 
    content['children'] = parse_children(section)
    
    return content

def parse_children(element):
    """é€’å½’è§£æå­å…ƒç´ """
    children = []
    
    for child in element.children:
        if child.name:  # æ˜¯æ ‡ç­¾
            child_data = {
                'tag': child.name,
                'attributes': dict(child.attrs),
                'text': child.get_text(strip=True) if not list(child.children) else '',
                'children': parse_children(child) if child.children else []
            }
            children.append(child_data)
        elif child.strip():  # æ˜¯æ–‡æœ¬èŠ‚ç‚¹
            children.append({
                'tag': 'text',
                'text': child.strip()
            })
    
    return children
```

---

### æ­¥éª¤3ï¼šæå–å…³é”®å†…å®¹

**æå–æ ‡é¢˜ã€æ®µè½ã€å›¾ç‰‡ã€é“¾æ¥ç­‰**

```python
def extract_section_content(soup):
    """æå–sectionçš„å…³é”®å†…å®¹"""
    section = soup.find('section')
    
    content = {
        'section_index': None,  # ç¨åå¡«å……
        'html_raw': str(section),
        'structure': {
            'headings': [],
            'paragraphs': [],
            'images': [],
            'links': [],
            'buttons': [],
            'lists': [],
            'forms': []
        }
    }
    
    # æå–æ ‡é¢˜
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        headings = section.find_all(tag)
        for h in headings:
            content['structure']['headings'].append({
                'level': tag,
                'text': h.get_text(strip=True),
                'class': h.get('class', [])
            })
    
    # æå–æ®µè½
    paragraphs = section.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text:  # åªä¿ç•™éç©ºæ®µè½
            content['structure']['paragraphs'].append({
                'text': text,
                'class': p.get('class', [])
            })
    
    # æå–å›¾ç‰‡
    images = section.find_all('img')
    for img in images:
        content['structure']['images'].append({
            'src': img.get('src', ''),
            'alt': img.get('alt', ''),
            'srcset': img.get('srcset', ''),
            'class': img.get('class', []),
            'data_attrs': {k: v for k, v in img.attrs.items() if k.startswith('data-')}
        })
    
    # æå–é“¾æ¥
    links = section.find_all('a')
    for link in links:
        content['structure']['links'].append({
            'href': link.get('href', ''),
            'text': link.get_text(strip=True),
            'class': link.get('class', []),
            'target': link.get('target', '')
        })
    
    # æå–æŒ‰é’®ï¼ˆåŒ…æ‹¬<button>å’Œclasså«btnçš„å…ƒç´ ï¼‰
    buttons = section.find_all(['button'])
    btn_classes = section.find_all(class_=lambda x: x and ('btn' in ' '.join(x).lower() or 'button' in ' '.join(x).lower()))
    
    for btn in list(set(buttons + btn_classes)):
        content['structure']['buttons'].append({
            'text': btn.get_text(strip=True),
            'class': btn.get('class', []),
            'type': btn.get('type', ''),
            'href': btn.get('href', '') if btn.name == 'a' else ''
        })
    
    # æå–åˆ—è¡¨
    lists = section.find_all(['ul', 'ol'])
    for lst in lists:
        items = [li.get_text(strip=True) for li in lst.find_all('li')]
        content['structure']['lists'].append({
            'type': lst.name,
            'items': items,
            'class': lst.get('class', [])
        })
    
    # æå–è¡¨å•
    forms = section.find_all('form')
    for form in forms:
        inputs = []
        for inp in form.find_all(['input', 'textarea', 'select']):
            inputs.append({
                'type': inp.get('type', inp.name),
                'name': inp.get('name', ''),
                'placeholder': inp.get('placeholder', ''),
                'class': inp.get('class', [])
            })
        
        content['structure']['forms'].append({
            'action': form.get('action', ''),
            'method': form.get('method', ''),
            'inputs': inputs
        })
    
    return content
```

---

### æ­¥éª¤4ï¼šå¤„ç†æ‰€æœ‰Section

**æ‰¹é‡å¤„ç†æ‰€æœ‰section**

```python
def process_all_sections(html_file, structure_file):
    """å¤„ç†æ‰€æœ‰section"""
    # åŠ è½½ç»“æ„åœ°å›¾
    with open(structure_file, 'r') as f:
        structure = json.load(f)
    
    sections_content = []
    
    for section_info in structure['sections_map']:
        print(f"å¤„ç† Section {section_info['index']}...")
        
        # è¯»å–HTML
        section_html = read_section_html(html_file, section_info)
        
        # è§£æå†…å®¹
        soup = BeautifulSoup(section_html, 'html.parser')
        content = extract_section_content(soup)
        
        # æ·»åŠ å…ƒæ•°æ®
        content['section_index'] = section_info['index']
        content['section_type'] = section_info['type']
        content['section_class'] = section_info['className']
        
        # ä¿å­˜å•ä¸ªsection
        output_file = f"section_{section_info['index']}_content.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(content, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ“ å·²ä¿å­˜: {output_file}")
        
        sections_content.append(content)
    
    # ä¿å­˜æ±‡æ€»
    with open('all_sections_content.json', 'w', encoding='utf-8') as f:
        json.dump(sections_content, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ å®Œæˆï¼å…±å¤„ç† {len(sections_content)} ä¸ªsection")
    
    return sections_content

# æ‰§è¡Œ
sections = process_all_sections('index.html', 'sections_structure.json')
```

---

## ğŸ“¤ è¾“å‡ºæ ¼å¼

### section_{i}_content.json

```json
{
  "section_index": 2,
  "section_type": "services",
  "section_class": "py-100",
  "html_raw": "<section class=\"py-100\">...</section>",
  "structure": {
    "headings": [
      {
        "level": "h2",
        "text": "æˆ‘ä»¬çš„æœåŠ¡",
        "class": ["text-30", "text-center"]
      }
    ],
    "paragraphs": [
      {
        "text": "æä¾›ä¸“ä¸šçš„ç½‘ç«™å¼€å‘æœåŠ¡ï¼Œè®©æ‚¨çš„ä¸šåŠ¡åœ¨çº¿ä¸Šè“¬å‹ƒå‘å±•ã€‚",
        "class": ["text-18", "text-gray"]
      }
    ],
    "images": [
      {
        "src": "https://cdn.example.com/image1.jpg",
        "alt": "æœåŠ¡å›¾ç‰‡",
        "srcset": "",
        "class": ["img-fluid"],
        "data_attrs": {
          "data-w-id": "abc123"
        }
      }
    ],
    "links": [
      {
        "href": "/services",
        "text": "äº†è§£æ›´å¤š",
        "class": ["link-primary"],
        "target": ""
      }
    ],
    "buttons": [
      {
        "text": "ç«‹å³å’¨è¯¢",
        "class": ["btn", "btn-primary"],
        "type": "",
        "href": "/contact"
      }
    ],
    "lists": [
      {
        "type": "ul",
        "items": [
          "ç½‘ç«™è®¾è®¡",
          "å‰ç«¯å¼€å‘",
          "åç«¯å¼€å‘",
          "SEOä¼˜åŒ–"
        ],
        "class": ["service-list"]
      }
    ],
    "forms": []
  }
}
```

---

## âœ… æ£€æŸ¥ç‚¹

å®Œæˆæœ¬é˜¶æ®µåï¼Œå¿…é¡»ç¡®è®¤ï¼š

- [ ] **æ‰€æœ‰sectionéƒ½å·²å¤„ç†**
  - [ ] section_1_content.json âœ“
  - [ ] section_2_content.json âœ“
  - [ ] ... 
  - [ ] section_N_content.json âœ“
  
- [ ] **æ¯ä¸ªJSONæ–‡ä»¶åŒ…å«å®Œæ•´å†…å®¹**
  - [ ] æ‰€æœ‰æ ‡é¢˜æ–‡å­—
  - [ ] æ‰€æœ‰æ®µè½æ–‡å­—
  - [ ] æ‰€æœ‰å›¾ç‰‡URL
  - [ ] æ‰€æœ‰é“¾æ¥href
  - [ ] æ‰€æœ‰æŒ‰é’®æ–‡å­—
  
- [ ] **æ— é—æ¼section**
  - [ ] æ–‡ä»¶æ•°é‡ = sectionæ€»æ•°
  
- [ ] **æ— å ä½ç¬¦**
  - [ ] æ²¡æœ‰ "Content goes here"
  - [ ] æ²¡æœ‰ "TODO"
  - [ ] æ²¡æœ‰ç©ºçš„æ–‡å­—å­—æ®µ

---

## ğŸš¨ å¸¸è§é”™è¯¯åŠé¿å…

### âŒ é”™è¯¯1ï¼šåªæå–äº†éƒ¨åˆ†section

**è¡¨ç°**ï¼š
```
åªç”Ÿæˆäº† section_1_content.json åˆ° section_5_content.json
ä½†å®é™…æœ‰15ä¸ªsection
```

**åŸå› **ï¼šå¤„ç†è¿‡ç¨‹ä¸­æ–­ï¼Œæˆ–å¾ªç¯æœ‰é—®é¢˜

**è§£å†³**ï¼š
```python
# æ·»åŠ è¿›åº¦è·Ÿè¸ª
total = len(structure['sections_map'])
for i, section_info in enumerate(structure['sections_map'], 1):
    print(f"[{i}/{total}] å¤„ç† Section {section_info['index']}...")
    # ... å¤„ç†ä»£ç 
```

---

### âŒ é”™è¯¯2ï¼šå›¾ç‰‡URLä¸å®Œæ•´

**è¡¨ç°**ï¼š
```json
{
  "images": [
    {
      "src": "images/photo.jpg"  // âŒ ç›¸å¯¹è·¯å¾„
    }
  ]
}
```

**è§£å†³**ï¼š
```python
def normalize_image_url(src, base_url=''):
    """è§„èŒƒåŒ–å›¾ç‰‡URL"""
    if src.startswith('http'):
        return src
    elif src.startswith('//'):
        return 'https:' + src
    elif src.startswith('/'):
        return base_url + src
    else:
        return base_url + '/' + src
```

---

### âŒ é”™è¯¯3ï¼šæ–‡å­—å†…å®¹æœ‰HTMLå®ä½“

**è¡¨ç°**ï¼š
```json
{
  "text": "We&amp;#x27;re the best"  // âŒ æœªè§£ç 
}
```

**è§£å†³**ï¼š
```python
import html

text = html.unescape(raw_text)
```

---

### âŒ é”™è¯¯4ï¼šåµŒå¥—ç»“æ„ä¸¢å¤±

**è¡¨ç°**ï¼šåªæå–äº†é¡¶å±‚æ–‡å­—ï¼Œä¸¢å¤±äº†åµŒå¥—çš„å¡ç‰‡ç»“æ„

**è§£å†³**ï¼šä½¿ç”¨é€’å½’è§£æï¼Œä¿ç•™å®Œæ•´çš„HTMLç»“æ„æ ‘

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### 1. å¤§sectionåˆ†å—å¤„ç†

å¦‚æœå•ä¸ªsection > 20KBï¼š
```python
def split_large_section(section_html, max_size=20000):
    """å°†å¤§sectionåˆ†æˆæ›´å°çš„å—"""
    # å¯ä»¥æŒ‰å­divåˆ†å‰²
    soup = BeautifulSoup(section_html, 'html.parser')
    section = soup.find('section')
    
    chunks = []
    current_chunk = []
    current_size = 0
    
    for child in section.children:
        child_html = str(child)
        child_size = len(child_html)
        
        if current_size + child_size > max_size and current_chunk:
            chunks.append(''.join(current_chunk))
            current_chunk = [child_html]
            current_size = child_size
        else:
            current_chunk.append(child_html)
            current_size += child_size
    
    if current_chunk:
        chunks.append(''.join(current_chunk))
    
    return chunks
```

### 2. å¹¶è¡Œå¤„ç†ï¼ˆå¦‚æœæœ‰å¤šä¸ªsectionï¼‰

```python
from multiprocessing import Pool

def process_section_wrapper(args):
    html_file, section_info = args
    # ... å¤„ç†ä»£ç 
    return content

# å¹¶è¡Œå¤„ç†
with Pool(processes=4) as pool:
    args = [(html_file, info) for info in structure['sections_map']]
    results = pool.map(process_section_wrapper, args)
```

---

[â† ä¸Šä¸€æ­¥ï¼šæ¨¡å—03 - è®¾è®¡ç³»ç»Ÿå½’çº³](./03_è®¾è®¡ç³»ç»Ÿå½’çº³.md) | [â†’ ä¸‹ä¸€æ­¥ï¼šæ¨¡å—05 - æ•°æ®åˆ†ç¦»](./05_æ•°æ®åˆ†ç¦».md)
